{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# Q.1\n",
        "아래 출력값을 갖도록 a 변수값을 설정 (Hint: np.arange(n) 활용)"
      ],
      "metadata": {
        "id": "V6wknBPTVL4I"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#a = ??\n",
        "import torch\n",
        "import numpy as np\n",
        "\n",
        "a = np.arange(24).reshape(2,3,4)\n",
        "\n",
        "print(\"Shape :\", a.shape)\n",
        "print(\"Value :\", a)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "rd0_sOqgVMJB",
        "outputId": "dda08107-52f5-4093-bcca-85ff122bfb8e"
      },
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Shape : (2, 3, 4)\n",
            "Value : [[[ 0  1  2  3]\n",
            "  [ 4  5  6  7]\n",
            "  [ 8  9 10 11]]\n",
            "\n",
            " [[12 13 14 15]\n",
            "  [16 17 18 19]\n",
            "  [20 21 22 23]]]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "#Output\n",
        "Shape : (2,3,4)\n",
        "- Value : [[[ 0 1 2 3]\n",
        "- [ 4 5 6 7]\n",
        "- [8 9 10 11]\n",
        "\n",
        "- [[12 13 14 15]\n",
        "- [16 17 18 19]\n",
        "*이탤릭체 텍스트*- [20 21 22 23]]]"
      ],
      "metadata": {
        "id": "6i0W2ULrWjPv"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Q.2\n",
        "Exercise 1의 a값과 slicing을 통해, 다음 두 값을 출력"
      ],
      "metadata": {
        "id": "KkhTqlA-bmsp"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#print(a[???]) #Problem1\n",
        "#print(a[???]) #Problem2\n",
        "\n",
        "print(a[0,1:3,2:4]) #Problem1\n",
        "print(a[0:1,1:3,2:4]) #Problem2 # 0:1을 사용해서 shape을 유지할 수 있다.\n",
        "# print(np.reshape(a[0,1:3,2:4], (1,2,2))) #Problem2"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "L1pDx-4gWTsT",
        "outputId": "9a813645-f486-4886-d0a7-eceab09d81fe"
      },
      "execution_count": 14,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[[ 6  7]\n",
            " [10 11]]\n",
            "[[[ 6  7]\n",
            "  [10 11]]]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Q.3\n",
        "test.csv 데이터를 활용하여, 70, 80, 90에 대한 Total Score 구하기 (nn.Module 활용) *Hint: numpy genfromtext 활용"
      ],
      "metadata": {
        "id": "ruyakuAGcGIE"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "data = open('test.csv', 'r')\n",
        "print(data)"
      ],
      "metadata": {
        "id": "2X2ngsSsaMtd",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "8749db97-d1b8-492c-b7ee-a7f5cfc08855"
      },
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "<_io.TextIOWrapper name='test.csv' mode='r' encoding='UTF-8'>\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import csv\n",
        "\n",
        "with open('test.csv' , 'r') as f :\n",
        "    dat = [k for k in csv.reader(f)]\n",
        "print(dat)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Xz7fEuJgtz1_",
        "outputId": "3dfc676d-7090-4f9d-efd3-31eaa57854fd"
      },
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[['73', '80', '75', '152'], ['93', '88', '93', '185'], ['89', '91', '90', '180'], ['96', '98', '100', '196'], ['73', '66', '70', '142'], ['53', '46', '55', '101'], ['69', '74', '77', '149'], ['47', '56', '60', '115'], ['87', '79', '90', '175'], ['79', '70', '88', '164'], ['69', '70', '73', '141'], ['70', '65', '74', '141'], ['93', '95', '91', '184'], ['79', '80', '73', '152'], ['70', '73', '78', '148'], ['93', '89', '96', '192'], ['78', '75', '68', '147'], ['81', '90', '93', '183'], ['88', '92', '86', '177'], ['78', '83', '77', '159'], ['82', '86', '90', '177'], ['86', '82', '89', '175'], ['78', '83', '85', '175'], ['76', '83', '71', '149'], ['96', '93', '95', '192']]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "\n",
        "data = np.loadtxt('test.csv' , delimiter=',' , skiprows = 1, dtype=float)\n",
        "print(data)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Jgepixj-usqO",
        "outputId": "4657cc70-c585-4219-f1fb-0c4cc57b2fd0"
      },
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[[ 93.  88.  93. 185.]\n",
            " [ 89.  91.  90. 180.]\n",
            " [ 96.  98. 100. 196.]\n",
            " [ 73.  66.  70. 142.]\n",
            " [ 53.  46.  55. 101.]\n",
            " [ 69.  74.  77. 149.]\n",
            " [ 47.  56.  60. 115.]\n",
            " [ 87.  79.  90. 175.]\n",
            " [ 79.  70.  88. 164.]\n",
            " [ 69.  70.  73. 141.]\n",
            " [ 70.  65.  74. 141.]\n",
            " [ 93.  95.  91. 184.]\n",
            " [ 79.  80.  73. 152.]\n",
            " [ 70.  73.  78. 148.]\n",
            " [ 93.  89.  96. 192.]\n",
            " [ 78.  75.  68. 147.]\n",
            " [ 81.  90.  93. 183.]\n",
            " [ 88.  92.  86. 177.]\n",
            " [ 78.  83.  77. 159.]\n",
            " [ 82.  86.  90. 177.]\n",
            " [ 86.  82.  89. 175.]\n",
            " [ 78.  83.  85. 175.]\n",
            " [ 76.  83.  71. 149.]\n",
            " [ 96.  93.  95. 192.]]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "\n",
        "data = np.genfromtxt('test.csv' , skip_header = 1 , delimiter = ',' , dtype = float)\n",
        "# data = np.loadtxt('test.csv' , delimiter=',' , skiprows = 1, dtype=float)\n",
        "print(data)\n",
        "print(\"-----\")\n",
        "print(data[0])\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "YUFTECecUW-Q",
        "outputId": "44b02b2a-af5c-4a3e-f238-0108bbcc4c1d"
      },
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[[ 93.  88.  93. 185.]\n",
            " [ 89.  91.  90. 180.]\n",
            " [ 96.  98. 100. 196.]\n",
            " [ 73.  66.  70. 142.]\n",
            " [ 53.  46.  55. 101.]\n",
            " [ 69.  74.  77. 149.]\n",
            " [ 47.  56.  60. 115.]\n",
            " [ 87.  79.  90. 175.]\n",
            " [ 79.  70.  88. 164.]\n",
            " [ 69.  70.  73. 141.]\n",
            " [ 70.  65.  74. 141.]\n",
            " [ 93.  95.  91. 184.]\n",
            " [ 79.  80.  73. 152.]\n",
            " [ 70.  73.  78. 148.]\n",
            " [ 93.  89.  96. 192.]\n",
            " [ 78.  75.  68. 147.]\n",
            " [ 81.  90.  93. 183.]\n",
            " [ 88.  92.  86. 177.]\n",
            " [ 78.  83.  77. 159.]\n",
            " [ 82.  86.  90. 177.]\n",
            " [ 86.  82.  89. 175.]\n",
            " [ 78.  83.  85. 175.]\n",
            " [ 76.  83.  71. 149.]\n",
            " [ 96.  93.  95. 192.]]\n",
            "-----\n",
            "[ 93.  88.  93. 185.]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "print(\"total score 70 counts :\",len(data[data==70]))\n",
        "print(\"total score 80 counts :\",len(data[data==80]))\n",
        "print(\"total score 90 counts :\",len(data[data==90]))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "GhmWUCJ4vseT",
        "outputId": "f1677f08-eeb0-4984-81de-1b8b84725755"
      },
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "total score 70 counts : 5\n",
            "total score 80 counts : 1\n",
            "total score 90 counts : 4\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "print(\"total score 70 up counts :\",len(data[data>=70]))\n",
        "print(\"total score 80 up counts :\",len(data[data>=80]))\n",
        "print(\"total score 90 up counts :\",len(data[data>=90]))"
      ],
      "metadata": {
        "id": "vl2PEMkov-ux",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "b430257f-43c2-4908-f73a-1333a7c6148c"
      },
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "total score 70 up counts : 85\n",
            "total score 80 up counts : 62\n",
            "total score 90 up counts : 44\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "import torch.optim as optim\n",
        "from numpy import genfromtxt\n",
        "\n",
        "# 크게 3단계. 데이터 load, 모델, 학습\n",
        "\n",
        "class MultivariateLinearRegressionModel(nn.Module):\n",
        "    def __init__(self):    # 모델이 어떻게 생겼는지. 초기화.\n",
        "        super().__init__()   \n",
        "        self.linear = nn.Linear(3, 1) # 입력 3개에 output 1개\n",
        "    def forward(self, x):\n",
        "        return self.linear(x)\n",
        "\n",
        "# data를 읽는 부분\n",
        "my_data = genfromtxt('test.csv', delimiter=',')\n",
        "x_train = torch.FloatTensor(my_data[:, :-1])\n",
        "y_train = torch.FloatTensor(my_data[:, -1:]) # -1만하면 차원이 유지되지 않는다.\n",
        "x_test = torch.FloatTensor([70, 80, 90])\n",
        "\n",
        "print(x_train)\n",
        "print(y_train)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "5-I7yiY0gbbk",
        "outputId": "93e1e7ce-8456-4832-df8a-5f4df12305e6"
      },
      "execution_count": 18,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "tensor([[ 73.,  80.,  75.],\n",
            "        [ 93.,  88.,  93.],\n",
            "        [ 89.,  91.,  90.],\n",
            "        [ 96.,  98., 100.],\n",
            "        [ 73.,  66.,  70.],\n",
            "        [ 53.,  46.,  55.],\n",
            "        [ 69.,  74.,  77.],\n",
            "        [ 47.,  56.,  60.],\n",
            "        [ 87.,  79.,  90.],\n",
            "        [ 79.,  70.,  88.],\n",
            "        [ 69.,  70.,  73.],\n",
            "        [ 70.,  65.,  74.],\n",
            "        [ 93.,  95.,  91.],\n",
            "        [ 79.,  80.,  73.],\n",
            "        [ 70.,  73.,  78.],\n",
            "        [ 93.,  89.,  96.],\n",
            "        [ 78.,  75.,  68.],\n",
            "        [ 81.,  90.,  93.],\n",
            "        [ 88.,  92.,  86.],\n",
            "        [ 78.,  83.,  77.],\n",
            "        [ 82.,  86.,  90.],\n",
            "        [ 86.,  82.,  89.],\n",
            "        [ 78.,  83.,  85.],\n",
            "        [ 76.,  83.,  71.],\n",
            "        [ 96.,  93.,  95.]])\n",
            "tensor([[152.],\n",
            "        [185.],\n",
            "        [180.],\n",
            "        [196.],\n",
            "        [142.],\n",
            "        [101.],\n",
            "        [149.],\n",
            "        [115.],\n",
            "        [175.],\n",
            "        [164.],\n",
            "        [141.],\n",
            "        [141.],\n",
            "        [184.],\n",
            "        [152.],\n",
            "        [148.],\n",
            "        [192.],\n",
            "        [147.],\n",
            "        [183.],\n",
            "        [177.],\n",
            "        [159.],\n",
            "        [177.],\n",
            "        [175.],\n",
            "        [175.],\n",
            "        [149.],\n",
            "        [192.]])\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "import torch.optim as optim\n",
        "from numpy import genfromtxt\n",
        "\n",
        "# 크게 3단계. 데이터 load, 모델, 학습\n",
        "\n",
        "class MultivariateLinearRegressionModel(nn.Module):\n",
        "    def __init__(self):    # 모델이 어떻게 생겼는지. 초기화.\n",
        "        super().__init__()   \n",
        "        self.linear = nn.Linear(3, 1) # 입력 3개에 output 1개 W1x1 + W2x2 + W3x3= y\n",
        "    def forward(self, x):    \n",
        "        return self.linear(x)\n",
        "\n",
        "# data를 읽는 부분\n",
        "my_data = genfromtxt('test.csv', delimiter=',')\n",
        "x_train = torch.FloatTensor(my_data[:, :-1])\n",
        "y_train = torch.FloatTensor(my_data[:, -1:]) # -1만하면 차원이 유지되지 않는다.\n",
        "x_test = torch.FloatTensor([70, 80, 90])\n",
        "\n",
        "# 모델 초기화\n",
        "model = MultivariateLinearRegressionModel()\n",
        "# optimizer 설정\n",
        "optimizer = optim.SGD(model.parameters(), lr=1e-5)\n",
        "nb_epochs = 500\n",
        "for epoch in range(nb_epochs + 1):\n",
        "    # H(x) 계산\n",
        "    prediction = model(x_train) # 자동으로 forward func. 실행됨. 모델이 예측한 값.\n",
        "    # cost 계산\n",
        "    cost = F.mse_loss(prediction, y_train) # 예측값, 실제값 차이 최소가 무엇인가.\n",
        "    # cost로 H(x) 개선\n",
        "    optimizer.zero_grad() # gradient descent 초기화\n",
        "    cost.backward()       # MSE를 gradient descent로 backward를 해라.\n",
        "    optimizer.step()      # 여기까지 하면 학습. 이렇게는 세트로 나옴.\n",
        "    # 20번마다 로그 출력\n",
        "    print('Epoch {:4d}/{} Cost: {:.6f}'.format(\n",
        "        epoch, nb_epochs, cost.item()\t\t#list(model.parameters())[0].data[0][0]\n",
        "    ))\n",
        "print(\"Answer : \", model(x_test).data[0]) # x_test 값 70 80 90을 넣었을 때 값이 무엇인가."
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "QRmJwCD2V-vG",
        "outputId": "d4280d29-4d7d-4042-ee1a-7681d0955d93"
      },
      "execution_count": 19,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch    0/500 Cost: 11901.990234\n",
            "Epoch    1/500 Cost: 4408.408691\n",
            "Epoch    2/500 Cost: 1637.820190\n",
            "Epoch    3/500 Cost: 613.453735\n",
            "Epoch    4/500 Cost: 234.713196\n",
            "Epoch    5/500 Cost: 94.679016\n",
            "Epoch    6/500 Cost: 42.900879\n",
            "Epoch    7/500 Cost: 23.753635\n",
            "Epoch    8/500 Cost: 16.670870\n",
            "Epoch    9/500 Cost: 14.048639\n",
            "Epoch   10/500 Cost: 13.075683\n",
            "Epoch   11/500 Cost: 12.712432\n",
            "Epoch   12/500 Cost: 12.574636\n",
            "Epoch   13/500 Cost: 12.520206\n",
            "Epoch   14/500 Cost: 12.496600\n",
            "Epoch   15/500 Cost: 12.484413\n",
            "Epoch   16/500 Cost: 12.476398\n",
            "Epoch   17/500 Cost: 12.470000\n",
            "Epoch   18/500 Cost: 12.464163\n",
            "Epoch   19/500 Cost: 12.458505\n",
            "Epoch   20/500 Cost: 12.452994\n",
            "Epoch   21/500 Cost: 12.447483\n",
            "Epoch   22/500 Cost: 12.441991\n",
            "Epoch   23/500 Cost: 12.436484\n",
            "Epoch   24/500 Cost: 12.431036\n",
            "Epoch   25/500 Cost: 12.425545\n",
            "Epoch   26/500 Cost: 12.420096\n",
            "Epoch   27/500 Cost: 12.414623\n",
            "Epoch   28/500 Cost: 12.409159\n",
            "Epoch   29/500 Cost: 12.403717\n",
            "Epoch   30/500 Cost: 12.398276\n",
            "Epoch   31/500 Cost: 12.392819\n",
            "Epoch   32/500 Cost: 12.387371\n",
            "Epoch   33/500 Cost: 12.381939\n",
            "Epoch   34/500 Cost: 12.376522\n",
            "Epoch   35/500 Cost: 12.371107\n",
            "Epoch   36/500 Cost: 12.365685\n",
            "Epoch   37/500 Cost: 12.360283\n",
            "Epoch   38/500 Cost: 12.354857\n",
            "Epoch   39/500 Cost: 12.349453\n",
            "Epoch   40/500 Cost: 12.344048\n",
            "Epoch   41/500 Cost: 12.338648\n",
            "Epoch   42/500 Cost: 12.333269\n",
            "Epoch   43/500 Cost: 12.327873\n",
            "Epoch   44/500 Cost: 12.322483\n",
            "Epoch   45/500 Cost: 12.317117\n",
            "Epoch   46/500 Cost: 12.311754\n",
            "Epoch   47/500 Cost: 12.306392\n",
            "Epoch   48/500 Cost: 12.301023\n",
            "Epoch   49/500 Cost: 12.295671\n",
            "Epoch   50/500 Cost: 12.290306\n",
            "Epoch   51/500 Cost: 12.284954\n",
            "Epoch   52/500 Cost: 12.279616\n",
            "Epoch   53/500 Cost: 12.274274\n",
            "Epoch   54/500 Cost: 12.268935\n",
            "Epoch   55/500 Cost: 12.263608\n",
            "Epoch   56/500 Cost: 12.258285\n",
            "Epoch   57/500 Cost: 12.252959\n",
            "Epoch   58/500 Cost: 12.247655\n",
            "Epoch   59/500 Cost: 12.242344\n",
            "Epoch   60/500 Cost: 12.237031\n",
            "Epoch   61/500 Cost: 12.231724\n",
            "Epoch   62/500 Cost: 12.226428\n",
            "Epoch   63/500 Cost: 12.221155\n",
            "Epoch   64/500 Cost: 12.215830\n",
            "Epoch   65/500 Cost: 12.210574\n",
            "Epoch   66/500 Cost: 12.205300\n",
            "Epoch   67/500 Cost: 12.200009\n",
            "Epoch   68/500 Cost: 12.194756\n",
            "Epoch   69/500 Cost: 12.189487\n",
            "Epoch   70/500 Cost: 12.184240\n",
            "Epoch   71/500 Cost: 12.178982\n",
            "Epoch   72/500 Cost: 12.173720\n",
            "Epoch   73/500 Cost: 12.168476\n",
            "Epoch   74/500 Cost: 12.163239\n",
            "Epoch   75/500 Cost: 12.158004\n",
            "Epoch   76/500 Cost: 12.152768\n",
            "Epoch   77/500 Cost: 12.147532\n",
            "Epoch   78/500 Cost: 12.142307\n",
            "Epoch   79/500 Cost: 12.137094\n",
            "Epoch   80/500 Cost: 12.131887\n",
            "Epoch   81/500 Cost: 12.126688\n",
            "Epoch   82/500 Cost: 12.121469\n",
            "Epoch   83/500 Cost: 12.116272\n",
            "Epoch   84/500 Cost: 12.111063\n",
            "Epoch   85/500 Cost: 12.105887\n",
            "Epoch   86/500 Cost: 12.100708\n",
            "Epoch   87/500 Cost: 12.095523\n",
            "Epoch   88/500 Cost: 12.090329\n",
            "Epoch   89/500 Cost: 12.085163\n",
            "Epoch   90/500 Cost: 12.079992\n",
            "Epoch   91/500 Cost: 12.074845\n",
            "Epoch   92/500 Cost: 12.069675\n",
            "Epoch   93/500 Cost: 12.064517\n",
            "Epoch   94/500 Cost: 12.059368\n",
            "Epoch   95/500 Cost: 12.054230\n",
            "Epoch   96/500 Cost: 12.049080\n",
            "Epoch   97/500 Cost: 12.043953\n",
            "Epoch   98/500 Cost: 12.038815\n",
            "Epoch   99/500 Cost: 12.033684\n",
            "Epoch  100/500 Cost: 12.028553\n",
            "Epoch  101/500 Cost: 12.023440\n",
            "Epoch  102/500 Cost: 12.018323\n",
            "Epoch  103/500 Cost: 12.013226\n",
            "Epoch  104/500 Cost: 12.008100\n",
            "Epoch  105/500 Cost: 12.003010\n",
            "Epoch  106/500 Cost: 11.997903\n",
            "Epoch  107/500 Cost: 11.992827\n",
            "Epoch  108/500 Cost: 11.987732\n",
            "Epoch  109/500 Cost: 11.982641\n",
            "Epoch  110/500 Cost: 11.977573\n",
            "Epoch  111/500 Cost: 11.972507\n",
            "Epoch  112/500 Cost: 11.967410\n",
            "Epoch  113/500 Cost: 11.962359\n",
            "Epoch  114/500 Cost: 11.957318\n",
            "Epoch  115/500 Cost: 11.952254\n",
            "Epoch  116/500 Cost: 11.947190\n",
            "Epoch  117/500 Cost: 11.942138\n",
            "Epoch  118/500 Cost: 11.937095\n",
            "Epoch  119/500 Cost: 11.932055\n",
            "Epoch  120/500 Cost: 11.927033\n",
            "Epoch  121/500 Cost: 11.922001\n",
            "Epoch  122/500 Cost: 11.916969\n",
            "Epoch  123/500 Cost: 11.911954\n",
            "Epoch  124/500 Cost: 11.906922\n",
            "Epoch  125/500 Cost: 11.901930\n",
            "Epoch  126/500 Cost: 11.896919\n",
            "Epoch  127/500 Cost: 11.891926\n",
            "Epoch  128/500 Cost: 11.886902\n",
            "Epoch  129/500 Cost: 11.881922\n",
            "Epoch  130/500 Cost: 11.876928\n",
            "Epoch  131/500 Cost: 11.871949\n",
            "Epoch  132/500 Cost: 11.866961\n",
            "Epoch  133/500 Cost: 11.861990\n",
            "Epoch  134/500 Cost: 11.857033\n",
            "Epoch  135/500 Cost: 11.852038\n",
            "Epoch  136/500 Cost: 11.847077\n",
            "Epoch  137/500 Cost: 11.842121\n",
            "Epoch  138/500 Cost: 11.837166\n",
            "Epoch  139/500 Cost: 11.832217\n",
            "Epoch  140/500 Cost: 11.827273\n",
            "Epoch  141/500 Cost: 11.822330\n",
            "Epoch  142/500 Cost: 11.817384\n",
            "Epoch  143/500 Cost: 11.812456\n",
            "Epoch  144/500 Cost: 11.807529\n",
            "Epoch  145/500 Cost: 11.802615\n",
            "Epoch  146/500 Cost: 11.797673\n",
            "Epoch  147/500 Cost: 11.792760\n",
            "Epoch  148/500 Cost: 11.787873\n",
            "Epoch  149/500 Cost: 11.782937\n",
            "Epoch  150/500 Cost: 11.778049\n",
            "Epoch  151/500 Cost: 11.773163\n",
            "Epoch  152/500 Cost: 11.768267\n",
            "Epoch  153/500 Cost: 11.763367\n",
            "Epoch  154/500 Cost: 11.758479\n",
            "Epoch  155/500 Cost: 11.753589\n",
            "Epoch  156/500 Cost: 11.748717\n",
            "Epoch  157/500 Cost: 11.743860\n",
            "Epoch  158/500 Cost: 11.738965\n",
            "Epoch  159/500 Cost: 11.734114\n",
            "Epoch  160/500 Cost: 11.729242\n",
            "Epoch  161/500 Cost: 11.724395\n",
            "Epoch  162/500 Cost: 11.719529\n",
            "Epoch  163/500 Cost: 11.714701\n",
            "Epoch  164/500 Cost: 11.709844\n",
            "Epoch  165/500 Cost: 11.705017\n",
            "Epoch  166/500 Cost: 11.700186\n",
            "Epoch  167/500 Cost: 11.695360\n",
            "Epoch  168/500 Cost: 11.690518\n",
            "Epoch  169/500 Cost: 11.685692\n",
            "Epoch  170/500 Cost: 11.680895\n",
            "Epoch  171/500 Cost: 11.676057\n",
            "Epoch  172/500 Cost: 11.671264\n",
            "Epoch  173/500 Cost: 11.666457\n",
            "Epoch  174/500 Cost: 11.661643\n",
            "Epoch  175/500 Cost: 11.656860\n",
            "Epoch  176/500 Cost: 11.652055\n",
            "Epoch  177/500 Cost: 11.647278\n",
            "Epoch  178/500 Cost: 11.642498\n",
            "Epoch  179/500 Cost: 11.637696\n",
            "Epoch  180/500 Cost: 11.632927\n",
            "Epoch  181/500 Cost: 11.628169\n",
            "Epoch  182/500 Cost: 11.623391\n",
            "Epoch  183/500 Cost: 11.618630\n",
            "Epoch  184/500 Cost: 11.613864\n",
            "Epoch  185/500 Cost: 11.609103\n",
            "Epoch  186/500 Cost: 11.604371\n",
            "Epoch  187/500 Cost: 11.599597\n",
            "Epoch  188/500 Cost: 11.594873\n",
            "Epoch  189/500 Cost: 11.590137\n",
            "Epoch  190/500 Cost: 11.585411\n",
            "Epoch  191/500 Cost: 11.580686\n",
            "Epoch  192/500 Cost: 11.575951\n",
            "Epoch  193/500 Cost: 11.571232\n",
            "Epoch  194/500 Cost: 11.566514\n",
            "Epoch  195/500 Cost: 11.561790\n",
            "Epoch  196/500 Cost: 11.557082\n",
            "Epoch  197/500 Cost: 11.552378\n",
            "Epoch  198/500 Cost: 11.547674\n",
            "Epoch  199/500 Cost: 11.543000\n",
            "Epoch  200/500 Cost: 11.538303\n",
            "Epoch  201/500 Cost: 11.533605\n",
            "Epoch  202/500 Cost: 11.528923\n",
            "Epoch  203/500 Cost: 11.524239\n",
            "Epoch  204/500 Cost: 11.519566\n",
            "Epoch  205/500 Cost: 11.514880\n",
            "Epoch  206/500 Cost: 11.510214\n",
            "Epoch  207/500 Cost: 11.505547\n",
            "Epoch  208/500 Cost: 11.500898\n",
            "Epoch  209/500 Cost: 11.496223\n",
            "Epoch  210/500 Cost: 11.491596\n",
            "Epoch  211/500 Cost: 11.486930\n",
            "Epoch  212/500 Cost: 11.482300\n",
            "Epoch  213/500 Cost: 11.477654\n",
            "Epoch  214/500 Cost: 11.473015\n",
            "Epoch  215/500 Cost: 11.468372\n",
            "Epoch  216/500 Cost: 11.463758\n",
            "Epoch  217/500 Cost: 11.459137\n",
            "Epoch  218/500 Cost: 11.454526\n",
            "Epoch  219/500 Cost: 11.449904\n",
            "Epoch  220/500 Cost: 11.445300\n",
            "Epoch  221/500 Cost: 11.440691\n",
            "Epoch  222/500 Cost: 11.436078\n",
            "Epoch  223/500 Cost: 11.431493\n",
            "Epoch  224/500 Cost: 11.426894\n",
            "Epoch  225/500 Cost: 11.422271\n",
            "Epoch  226/500 Cost: 11.417706\n",
            "Epoch  227/500 Cost: 11.413120\n",
            "Epoch  228/500 Cost: 11.408538\n",
            "Epoch  229/500 Cost: 11.403973\n",
            "Epoch  230/500 Cost: 11.399402\n",
            "Epoch  231/500 Cost: 11.394848\n",
            "Epoch  232/500 Cost: 11.390253\n",
            "Epoch  233/500 Cost: 11.385716\n",
            "Epoch  234/500 Cost: 11.381164\n",
            "Epoch  235/500 Cost: 11.376617\n",
            "Epoch  236/500 Cost: 11.372049\n",
            "Epoch  237/500 Cost: 11.367517\n",
            "Epoch  238/500 Cost: 11.362984\n",
            "Epoch  239/500 Cost: 11.358445\n",
            "Epoch  240/500 Cost: 11.353916\n",
            "Epoch  241/500 Cost: 11.349381\n",
            "Epoch  242/500 Cost: 11.344856\n",
            "Epoch  243/500 Cost: 11.340341\n",
            "Epoch  244/500 Cost: 11.335829\n",
            "Epoch  245/500 Cost: 11.331318\n",
            "Epoch  246/500 Cost: 11.326817\n",
            "Epoch  247/500 Cost: 11.322313\n",
            "Epoch  248/500 Cost: 11.317793\n",
            "Epoch  249/500 Cost: 11.313313\n",
            "Epoch  250/500 Cost: 11.308822\n",
            "Epoch  251/500 Cost: 11.304338\n",
            "Epoch  252/500 Cost: 11.299853\n",
            "Epoch  253/500 Cost: 11.295358\n",
            "Epoch  254/500 Cost: 11.290893\n",
            "Epoch  255/500 Cost: 11.286415\n",
            "Epoch  256/500 Cost: 11.281948\n",
            "Epoch  257/500 Cost: 11.277469\n",
            "Epoch  258/500 Cost: 11.273023\n",
            "Epoch  259/500 Cost: 11.268563\n",
            "Epoch  260/500 Cost: 11.264113\n",
            "Epoch  261/500 Cost: 11.259668\n",
            "Epoch  262/500 Cost: 11.255222\n",
            "Epoch  263/500 Cost: 11.250784\n",
            "Epoch  264/500 Cost: 11.246342\n",
            "Epoch  265/500 Cost: 11.241907\n",
            "Epoch  266/500 Cost: 11.237493\n",
            "Epoch  267/500 Cost: 11.233066\n",
            "Epoch  268/500 Cost: 11.228649\n",
            "Epoch  269/500 Cost: 11.224224\n",
            "Epoch  270/500 Cost: 11.219812\n",
            "Epoch  271/500 Cost: 11.215396\n",
            "Epoch  272/500 Cost: 11.210996\n",
            "Epoch  273/500 Cost: 11.206583\n",
            "Epoch  274/500 Cost: 11.202197\n",
            "Epoch  275/500 Cost: 11.197810\n",
            "Epoch  276/500 Cost: 11.193428\n",
            "Epoch  277/500 Cost: 11.189017\n",
            "Epoch  278/500 Cost: 11.184645\n",
            "Epoch  279/500 Cost: 11.180272\n",
            "Epoch  280/500 Cost: 11.175882\n",
            "Epoch  281/500 Cost: 11.171521\n",
            "Epoch  282/500 Cost: 11.167149\n",
            "Epoch  283/500 Cost: 11.162785\n",
            "Epoch  284/500 Cost: 11.158418\n",
            "Epoch  285/500 Cost: 11.154087\n",
            "Epoch  286/500 Cost: 11.149716\n",
            "Epoch  287/500 Cost: 11.145363\n",
            "Epoch  288/500 Cost: 11.141036\n",
            "Epoch  289/500 Cost: 11.136683\n",
            "Epoch  290/500 Cost: 11.132350\n",
            "Epoch  291/500 Cost: 11.128020\n",
            "Epoch  292/500 Cost: 11.123685\n",
            "Epoch  293/500 Cost: 11.119358\n",
            "Epoch  294/500 Cost: 11.115047\n",
            "Epoch  295/500 Cost: 11.110708\n",
            "Epoch  296/500 Cost: 11.106414\n",
            "Epoch  297/500 Cost: 11.102098\n",
            "Epoch  298/500 Cost: 11.097800\n",
            "Epoch  299/500 Cost: 11.093489\n",
            "Epoch  300/500 Cost: 11.089180\n",
            "Epoch  301/500 Cost: 11.084888\n",
            "Epoch  302/500 Cost: 11.080605\n",
            "Epoch  303/500 Cost: 11.076321\n",
            "Epoch  304/500 Cost: 11.072029\n",
            "Epoch  305/500 Cost: 11.067754\n",
            "Epoch  306/500 Cost: 11.063460\n",
            "Epoch  307/500 Cost: 11.059196\n",
            "Epoch  308/500 Cost: 11.054932\n",
            "Epoch  309/500 Cost: 11.050665\n",
            "Epoch  310/500 Cost: 11.046404\n",
            "Epoch  311/500 Cost: 11.042153\n",
            "Epoch  312/500 Cost: 11.037897\n",
            "Epoch  313/500 Cost: 11.033643\n",
            "Epoch  314/500 Cost: 11.029394\n",
            "Epoch  315/500 Cost: 11.025158\n",
            "Epoch  316/500 Cost: 11.020913\n",
            "Epoch  317/500 Cost: 11.016682\n",
            "Epoch  318/500 Cost: 11.012454\n",
            "Epoch  319/500 Cost: 11.008227\n",
            "Epoch  320/500 Cost: 11.003997\n",
            "Epoch  321/500 Cost: 10.999784\n",
            "Epoch  322/500 Cost: 10.995570\n",
            "Epoch  323/500 Cost: 10.991345\n",
            "Epoch  324/500 Cost: 10.987146\n",
            "Epoch  325/500 Cost: 10.982942\n",
            "Epoch  326/500 Cost: 10.978732\n",
            "Epoch  327/500 Cost: 10.974527\n",
            "Epoch  328/500 Cost: 10.970341\n",
            "Epoch  329/500 Cost: 10.966152\n",
            "Epoch  330/500 Cost: 10.961970\n",
            "Epoch  331/500 Cost: 10.957777\n",
            "Epoch  332/500 Cost: 10.953591\n",
            "Epoch  333/500 Cost: 10.949410\n",
            "Epoch  334/500 Cost: 10.945244\n",
            "Epoch  335/500 Cost: 10.941076\n",
            "Epoch  336/500 Cost: 10.936912\n",
            "Epoch  337/500 Cost: 10.932742\n",
            "Epoch  338/500 Cost: 10.928588\n",
            "Epoch  339/500 Cost: 10.924427\n",
            "Epoch  340/500 Cost: 10.920284\n",
            "Epoch  341/500 Cost: 10.916152\n",
            "Epoch  342/500 Cost: 10.911990\n",
            "Epoch  343/500 Cost: 10.907832\n",
            "Epoch  344/500 Cost: 10.903728\n",
            "Epoch  345/500 Cost: 10.899594\n",
            "Epoch  346/500 Cost: 10.895454\n",
            "Epoch  347/500 Cost: 10.891336\n",
            "Epoch  348/500 Cost: 10.887202\n",
            "Epoch  349/500 Cost: 10.883105\n",
            "Epoch  350/500 Cost: 10.878977\n",
            "Epoch  351/500 Cost: 10.874864\n",
            "Epoch  352/500 Cost: 10.870749\n",
            "Epoch  353/500 Cost: 10.866663\n",
            "Epoch  354/500 Cost: 10.862539\n",
            "Epoch  355/500 Cost: 10.858469\n",
            "Epoch  356/500 Cost: 10.854365\n",
            "Epoch  357/500 Cost: 10.850284\n",
            "Epoch  358/500 Cost: 10.846184\n",
            "Epoch  359/500 Cost: 10.842098\n",
            "Epoch  360/500 Cost: 10.838038\n",
            "Epoch  361/500 Cost: 10.833953\n",
            "Epoch  362/500 Cost: 10.829876\n",
            "Epoch  363/500 Cost: 10.825827\n",
            "Epoch  364/500 Cost: 10.821756\n",
            "Epoch  365/500 Cost: 10.817696\n",
            "Epoch  366/500 Cost: 10.813623\n",
            "Epoch  367/500 Cost: 10.809587\n",
            "Epoch  368/500 Cost: 10.805536\n",
            "Epoch  369/500 Cost: 10.801487\n",
            "Epoch  370/500 Cost: 10.797450\n",
            "Epoch  371/500 Cost: 10.793397\n",
            "Epoch  372/500 Cost: 10.789362\n",
            "Epoch  373/500 Cost: 10.785316\n",
            "Epoch  374/500 Cost: 10.781311\n",
            "Epoch  375/500 Cost: 10.777273\n",
            "Epoch  376/500 Cost: 10.773259\n",
            "Epoch  377/500 Cost: 10.769236\n",
            "Epoch  378/500 Cost: 10.765221\n",
            "Epoch  379/500 Cost: 10.761211\n",
            "Epoch  380/500 Cost: 10.757208\n",
            "Epoch  381/500 Cost: 10.753207\n",
            "Epoch  382/500 Cost: 10.749192\n",
            "Epoch  383/500 Cost: 10.745206\n",
            "Epoch  384/500 Cost: 10.741223\n",
            "Epoch  385/500 Cost: 10.737218\n",
            "Epoch  386/500 Cost: 10.733254\n",
            "Epoch  387/500 Cost: 10.729257\n",
            "Epoch  388/500 Cost: 10.725273\n",
            "Epoch  389/500 Cost: 10.721300\n",
            "Epoch  390/500 Cost: 10.717332\n",
            "Epoch  391/500 Cost: 10.713354\n",
            "Epoch  392/500 Cost: 10.709373\n",
            "Epoch  393/500 Cost: 10.705420\n",
            "Epoch  394/500 Cost: 10.701460\n",
            "Epoch  395/500 Cost: 10.697515\n",
            "Epoch  396/500 Cost: 10.693546\n",
            "Epoch  397/500 Cost: 10.689605\n",
            "Epoch  398/500 Cost: 10.685659\n",
            "Epoch  399/500 Cost: 10.681728\n",
            "Epoch  400/500 Cost: 10.677788\n",
            "Epoch  401/500 Cost: 10.673853\n",
            "Epoch  402/500 Cost: 10.669934\n",
            "Epoch  403/500 Cost: 10.665998\n",
            "Epoch  404/500 Cost: 10.662073\n",
            "Epoch  405/500 Cost: 10.658137\n",
            "Epoch  406/500 Cost: 10.654233\n",
            "Epoch  407/500 Cost: 10.650311\n",
            "Epoch  408/500 Cost: 10.646402\n",
            "Epoch  409/500 Cost: 10.642509\n",
            "Epoch  410/500 Cost: 10.638580\n",
            "Epoch  411/500 Cost: 10.634705\n",
            "Epoch  412/500 Cost: 10.630806\n",
            "Epoch  413/500 Cost: 10.626906\n",
            "Epoch  414/500 Cost: 10.623016\n",
            "Epoch  415/500 Cost: 10.619140\n",
            "Epoch  416/500 Cost: 10.615230\n",
            "Epoch  417/500 Cost: 10.611371\n",
            "Epoch  418/500 Cost: 10.607487\n",
            "Epoch  419/500 Cost: 10.603617\n",
            "Epoch  420/500 Cost: 10.599752\n",
            "Epoch  421/500 Cost: 10.595883\n",
            "Epoch  422/500 Cost: 10.592023\n",
            "Epoch  423/500 Cost: 10.588153\n",
            "Epoch  424/500 Cost: 10.584303\n",
            "Epoch  425/500 Cost: 10.580450\n",
            "Epoch  426/500 Cost: 10.576609\n",
            "Epoch  427/500 Cost: 10.572766\n",
            "Epoch  428/500 Cost: 10.568912\n",
            "Epoch  429/500 Cost: 10.565085\n",
            "Epoch  430/500 Cost: 10.561226\n",
            "Epoch  431/500 Cost: 10.557407\n",
            "Epoch  432/500 Cost: 10.553564\n",
            "Epoch  433/500 Cost: 10.549747\n",
            "Epoch  434/500 Cost: 10.545924\n",
            "Epoch  435/500 Cost: 10.542114\n",
            "Epoch  436/500 Cost: 10.538300\n",
            "Epoch  437/500 Cost: 10.534472\n",
            "Epoch  438/500 Cost: 10.530670\n",
            "Epoch  439/500 Cost: 10.526865\n",
            "Epoch  440/500 Cost: 10.523067\n",
            "Epoch  441/500 Cost: 10.519243\n",
            "Epoch  442/500 Cost: 10.515459\n",
            "Epoch  443/500 Cost: 10.511665\n",
            "Epoch  444/500 Cost: 10.507891\n",
            "Epoch  445/500 Cost: 10.504085\n",
            "Epoch  446/500 Cost: 10.500300\n",
            "Epoch  447/500 Cost: 10.496532\n",
            "Epoch  448/500 Cost: 10.492767\n",
            "Epoch  449/500 Cost: 10.488992\n",
            "Epoch  450/500 Cost: 10.485229\n",
            "Epoch  451/500 Cost: 10.481449\n",
            "Epoch  452/500 Cost: 10.477688\n",
            "Epoch  453/500 Cost: 10.473915\n",
            "Epoch  454/500 Cost: 10.470173\n",
            "Epoch  455/500 Cost: 10.466414\n",
            "Epoch  456/500 Cost: 10.462660\n",
            "Epoch  457/500 Cost: 10.458936\n",
            "Epoch  458/500 Cost: 10.455187\n",
            "Epoch  459/500 Cost: 10.451430\n",
            "Epoch  460/500 Cost: 10.447710\n",
            "Epoch  461/500 Cost: 10.443969\n",
            "Epoch  462/500 Cost: 10.440250\n",
            "Epoch  463/500 Cost: 10.436507\n",
            "Epoch  464/500 Cost: 10.432800\n",
            "Epoch  465/500 Cost: 10.429080\n",
            "Epoch  466/500 Cost: 10.425355\n",
            "Epoch  467/500 Cost: 10.421645\n",
            "Epoch  468/500 Cost: 10.417920\n",
            "Epoch  469/500 Cost: 10.414211\n",
            "Epoch  470/500 Cost: 10.410518\n",
            "Epoch  471/500 Cost: 10.406824\n",
            "Epoch  472/500 Cost: 10.403125\n",
            "Epoch  473/500 Cost: 10.399440\n",
            "Epoch  474/500 Cost: 10.395742\n",
            "Epoch  475/500 Cost: 10.392054\n",
            "Epoch  476/500 Cost: 10.388371\n",
            "Epoch  477/500 Cost: 10.384692\n",
            "Epoch  478/500 Cost: 10.381009\n",
            "Epoch  479/500 Cost: 10.377344\n",
            "Epoch  480/500 Cost: 10.373662\n",
            "Epoch  481/500 Cost: 10.370012\n",
            "Epoch  482/500 Cost: 10.366330\n",
            "Epoch  483/500 Cost: 10.362671\n",
            "Epoch  484/500 Cost: 10.359012\n",
            "Epoch  485/500 Cost: 10.355358\n",
            "Epoch  486/500 Cost: 10.351716\n",
            "Epoch  487/500 Cost: 10.348048\n",
            "Epoch  488/500 Cost: 10.344420\n",
            "Epoch  489/500 Cost: 10.340769\n",
            "Epoch  490/500 Cost: 10.337122\n",
            "Epoch  491/500 Cost: 10.333485\n",
            "Epoch  492/500 Cost: 10.329861\n",
            "Epoch  493/500 Cost: 10.326229\n",
            "Epoch  494/500 Cost: 10.322601\n",
            "Epoch  495/500 Cost: 10.318975\n",
            "Epoch  496/500 Cost: 10.315364\n",
            "Epoch  497/500 Cost: 10.311754\n",
            "Epoch  498/500 Cost: 10.308136\n",
            "Epoch  499/500 Cost: 10.304500\n",
            "Epoch  500/500 Cost: 10.300911\n",
            "Answer :  tensor(164.6203)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "GbtE5KZAfYtl"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}